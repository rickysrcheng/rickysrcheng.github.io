---
layout: post
title: "[P] The Reversible Residual Network"
date: 2024-04-26 19:37:00 +0800
description: Notes about RevNet
tags: machine-learning backpropagation reversible-architecture
categories: machine-learning paper-reading
related_posts: false
pseudocode: true
---

## Motivation
- ResNet advanced the state of deep learning and allowed for much deeper and wider deep neural networks to be trained
- However, as networks get bigger, memory consumption increases due to the need to store layer activations during forward pass for gradient updates in backpropagation
- May be beneficial to trade compute time for memory space, especially in systems where GPU memory is limited or complex to manage (such as multi-GPU distributed training)

## Background Materials
#### Backpropagation and Saving Activation Values
- Backpropagation is a technique used to compute gradient of a cost function with respect to a network node. Essentially, backpropagation *propagates* the error gradient from the cost function back into the gradient of the network. These gradients are then used by an optimization algorithm, such as stochastic gradient descent, to update the network parameters. The details of backpropagation can be found in numerous online sources, thus I won't go into details here.
- The main issue the paper tries to tackle is that gradients of activation values require either the activation values or pre-activation values to compute
    - The popular ReLU activation function $$g(x) = \text{max}(0, x)$$ has the derivative $$g'(x) = \begin{cases} 0 & x < 0 \\ 1 & x > 0\end{cases}$$. This requires knowledge of the pre-activation values $$x$$.
- A technique used to reduce the amount of memory used to save activation values is called *gradient checkpointing*, which saves activation values every few layers and recomputes activation values for layers that don't have it saved. This trades memory for compute time.

#### ResNet
- Introduced in 2016 by [He et al](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf), ResNet is an architecture that uses residual learning blocks that made it possible to train deeper networks. 
<div class="row mt-3 justify-content-center">
    <div class="col-sm mt-3 mt-md-0" style="max-width:500px;">
        {% include figure.liquid loading="eager" path="assets/img/revnet-resnet.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

- ResNets are built out of residual blocks, which takes the form of 

    $$
    y = x + \mathcal{F}(x)
    $$

    where $$\mathcal{F}(x)$$ is called a residual function, typically a small and shallow neural network. The skip connection allows information to flow freely through the network, which alleviates the vanishing gradient problem.

#### Reversible Architectures
- Reversible architectures are models that can return the input given only the model itself and the outputs, i.e. $$y = f(x)$$ and $$x = f^{-1}(y)$$. 
- The architecture that inspired the authors is from [non-linear independent components estimation (NICE)](https://arxiv.org/pdf/1410.8516.pdf). The 
- Given $$x$$, we partition $$x$$ into $$(x_1, x_2)$$ along some dimension. Then we get the following forward mapping:

    $$
    \begin{align*}
    y_1 & = x_1 \\
    y_2 & = x_2 + \mathcal{F}(x_1)
    \end{align*}
    $$

    We can get the input $$x_1, x_2$$ back if we have the outputs $$y_1, y_2$$ using the following inverse mapping:

    $$
    \begin{align*}
    x_1 & = y_1 \\
    x_2 & = y_2 - \mathcal{F}(y_1)
    \end{align*}
    $$    

## Reversible ResNet
- In [RevNet](https://arxiv.org/abs/1707.04585), the authors proposed incorporating a reversible architecture into ResNet. The reversibility allows each block's pre-activations to be calculated directly from the next layer's activations, thereby saving memory.

####  Architecture
<div class="row mt-3 justify-content-center">
    <div class="col-sm mt-3 mt-md-0" style="max-width:550px;">
        {% include figure.liquid loading="eager" path="assets/img/revnet-arch.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

- RevNet is built out of reversible residual blocks. Given an input $$x$$, we first partition $$x$$ into $$(x_1, x_2)$$ along a dimension (the paper chose to partition along the channel). The resulting output is given as: 

    $$
    \begin{align*}
    y_1 & = x_1 + \mathcal{F}(x_2)\\
    y_2 & = x_2 + \mathcal{G}(y_1)
    \end{align*}
    $$

  where the functions $$\mathcal{F}(\cdot)$$ and $$\mathcal{G}(\cdot)$$ are analogous to the residual functions found in ResNet.

  The inverse mapping, similar to NICE, is given by:

    $$
    \begin{align*}
    x_1 & = y_1 - \mathcal{F}(x_2)\\
    x_2 & = y_2 - \mathcal{G}(y_1)
    \end{align*}
    $$

- Unlike residual blocks, reversible blocks must have a stride of 1. Otherwise information is discarded, which makes the layer irreversible. Activations still needs to be explicitly stored for non-reversible layers.

#### Backpropagation 
- The backpropagation for RevNet is given below. The paper used $$ \bar{v}_i$$ to denote the total derivative of node $$v_i$$ with respect to the cost function $$\mathcal{C}$$, $$ \bar{v}_i = d\mathcal{C}/d v_i$$.
```pseudocode
\begin{algorithm}
\caption{RevNet Backprop}
\begin{algorithmic}
\PROCEDURE{RevNetBackprop}{$ (y_1, y_2), (\bar{y}_1, \bar{y}_2) $}
    \STATE \COMMENT{Retrieve input activation}
    \STATE $x_2 \gets y_2 - \mathcal{G}(y_1)$
    \STATE $x_1 \gets y_1 - \mathcal{F}(x_2)$
    \STATE \COMMENT{Calculate activation gradient}
    \STATE $\bar{x}_1 \gets \bar{y}_1 + (\frac{\partial \mathcal{G}}{\partial y_1})^\intercal\bar{y}_2$
    \STATE $\bar{x}_2 \gets \bar{y}_2 + (\frac{\partial \mathcal{F}}{\partial y_2})^\intercal \bar{x}_1$
    \STATE \COMMENT{Calculate parameter gradient}
    \STATE $\bar{\theta}_\mathcal{F} \gets (\frac{\partial \mathcal{F}}{\partial \theta_{\mathcal{F}}})^\intercal \bar{x}_1$
    \STATE $\bar{\theta}_\mathcal{G} \gets (\frac{\partial \mathcal{G}}{\partial \theta_{\mathcal{G}}})^\intercal \bar{y}_2$
    \RETURN $(x_1, x_2), (\bar{x}_1, \bar{x}_2), (\bar{\theta}_\mathcal{F}, \bar{\theta}_\mathcal{G})$
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
```

- In contrast, the algorithm of ResNet is given below. Note that it requires $$x$$ be given as input:
```pseudocode
\begin{algorithm}
\caption{RevNet Backprop}
\begin{algorithmic}
\PROCEDURE{RevNetBackprop}{$ x, y, \bar{y} $}
    \STATE $\bar{x} \gets \bar{y} + (\frac{\partial F}{\partial x})^\intercal \bar{y}$
    \STATE $\bar{\theta}_\mathcal{F} \gets (\frac{\partial F}{\partial \theta_\mathcal{F}})^\intercal \bar{y}$
    \RETURN $\bar{x}, \bar{\theta}_\mathcal{F}$
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
```

## Experiments
- Since RevNet blocks have around twice the computation depth of a ResNet block. Thus, the RevNet models have approximately half the number of blocks and twice the number of channels than its ResNet counterpart.
- The experiments showed that RevNet performs similarly to ResNet in all of its classification tests and also matches the training dynamics.
- The paper doesn't cite a figure for how much memory saving was achieved. However, the authors did claim that, using the same GPU resources, RevNet was able to train on a mini-batch size of 128 images, whereas its ResNet counterpart was only able to train on mini-batch size of 32 images. 

## References
- The Reversible Residual Network: Backpropagation Without Storing Activations, Gomez et al., NeurIPS 2017
- NICE: Non-linear Independent Components Estimation, Dinh et al., ICLR 2015
- Deep Residual Learning for Image Recognition, He et al., CVPR 2016

