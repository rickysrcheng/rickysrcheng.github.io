<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://rickysrcheng.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rickysrcheng.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-04-18T10:45:04+00:00</updated><id>https://rickysrcheng.github.io/feed.xml</id><title type="html">Ricky Cheng</title><subtitle></subtitle><entry><title type="html">[P] PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications</title><link href="https://rickysrcheng.github.io/blog/2024/pipeswitch/" rel="alternate" type="text/html" title="[P] PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications"/><published>2024-03-21T21:30:00+00:00</published><updated>2024-03-21T21:30:00+00:00</updated><id>https://rickysrcheng.github.io/blog/2024/pipeswitch</id><content type="html" xml:base="https://rickysrcheng.github.io/blog/2024/pipeswitch/"><![CDATA[<h2 id="motivation">Motivation</h2> <ul> <li>Deep learning tasks have two primary workloads: training and inference. <ul> <li>Training workloads are throughput intensive but are more flexible in latency, since it typically takes a while to finish.</li> <li>Inference workloads are latency sensitive and have uncertain workloads.</li> </ul> </li> <li>Typical practice is to provision separate GPU clusters for training and inference, which leads to inefficiencies <ul> <li>Inference clusters are often over-provisioned to satisfy peak workload</li> <li>Training workloads cannot use inference clusters when inference load is low</li> </ul> </li> <li>Ideally, unify resources and use one cluster for both tasks via time-sharing <ul> <li>Problem: overhead for context switching is high for GPUs, which impacts inference latency requirements</li> </ul> </li> </ul> <h2 id="pipeswitch">PipeSwitch</h2> <ul> <li>PipeSwitch is a system proposed by <a href="https://www.usenix.org/conference/osdi20/presentation/bai">Zhihao Bai et al</a> to allow efficient time-sharing of a GPU</li> </ul> <div class="row mt-3 justify-content-center"> <div class="col-sm mt-3 mt-md-0" style="max-width:400px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pipeswitch-arch-480.webp 480w,/assets/img/pipeswitch-arch-800.webp 800w,/assets/img/pipeswitch-arch-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/pipeswitch-arch.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> PipeSwitch Architecture </div> <ul> <li>PipeSwitch consists of the following subsystems: <ul> <li>Controller: Responsible for handling client requests, scheduling workers to the GPU, and directing the memory daemon to allocate and transfer model weights</li> <li>Memory Daemon: Allocates GPU memory to the active worker and transfers model weights between host and GPU memory</li> <li>Active/Standby workers: Active workers are workers currently executing a task in the GPU. Standby workers are workers are workers who may be idle, initializing a new task, or cleaning up a previous task</li> </ul> </li> </ul> <h2 id="design-mechanisms">Design Mechanisms</h2> <p>PipeSwitch introduces three main mechanisms that work together to reduce context switching overhead</p> <h4 id="pipelined-model-transmisson">Pipelined Model Transmisson</h4> <div class="row mt-3 justify-content-center"> <div class="col-sm mt-3 mt-md-0" style="max-width:450px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pipeswitch-pipeline-480.webp 480w,/assets/img/pipeswitch-pipeline-800.webp 800w,/assets/img/pipeswitch-pipeline-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/pipeswitch-pipeline.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Pipelined model transmission can shorten latency and have better utilization of GPU resources </div> <ul> <li>Deep learning models employ a layered architecture which means that not all layer weights need to be loaded into memory to begin execution. This observation means that we can pipeline the model execution and data transmission <ul> <li>However, GPU cores executes calculations much faster than memory operations. Thus, need to consider the granularity. <ul> <li>Whole model granularity is the same as loading the whole model into memory before execution.</li> <li>Per layer granularity operates on one layer at a time. However, this may incur significant overhead due to PCIe costs and synchronization costs.</li> </ul> </li> </ul> </li> <li>To this end, the authors propose <em>model-aware grouping</em> to find the optimal grouping to transmit and execute. The algorithm considers both the layer and number of layers to group together. The algorithm and proof, which I will not go into, is provided in the paper for those who are interested.</li> </ul> <h4 id="unified-memory-management">Unified Memory Management</h4> <ul> <li> <p>Since DL tasks require GPU memory, the authors proposed a unified memory management system to reduce overhead from allocating and transmitting the models. This is because the memory footprint of the model parameters remain unchanged during task execution. Neither forward pass nor backpropagation will change the model structure. In addition, while intermediate results are needed during training, they are produced and consumed in a structured and predictable manner. Thus, a general purpose memory management system, like what CUDA provides, is too heavyweight.</p> </li> <li> <p>The unified memory management has four mechanisms that helps to reduce memory overhead:</p> <ol> <li><strong>Minimize Memory Allocation Overhead</strong>: Essentially, the memory daemon sits on top of CUDA and obtains GPU memory on startup. This eliminates the need for each task worker to allocate its own GPU memory. Instead, the daemon, having already obtained GPU memory, will pass a pointer to the worker, saving overhead. This also allows the daemon to guarantee memory isolation between workers.</li> <li><strong>Minimize memory footprint and avoid extra memory copies</strong>: It may be a case that the same model is needed for multiple tasks. Having each task have a duplicate copy wastes memory spaces. However, having a separate process to save models in host memory would incur memory overhead from transferring the model to the task to transmit to the GPU. The memory daemon solves both as it keeps one copy of each model in host memory. Since it manages both host and GPU memory, it can also transmit the model to GPU directly.</li> <li><strong>Minimize IPC Overhead</strong>: I’m not too familiar with GPU IPCs and the associated overheads; though I still tried my best in understanding this mechanism. <br/> Since the model transmission is pipelined, synchronization needs to occur between the memory daemon and the worker. However, using GPU IPC is expensive. However, the authors observed a property that memory allocation for a neural network model is <em>deterministic</em>. So, given the same model and GPU memory region, the memory pointers for each pipeline group would be the same as long as the allocation order is the same between the memory daemon and the worker. Thus, we can take advantage of this and use CPU IPCs, which are cheap, in place of GPU IPCs to signal which pipeline group is transmitted.</li> <li><strong>Pin Memory</strong>: GPUs require a page to be pinned in host memory for memory transmission. If no page is pinned in host memory, a temporary page is pinned for transmission. The authors thus pin the pages of the memory daemon to avoid such overhead.</li> </ol> </li> </ul> <h4 id="active-standby-worker-switching">Active-Standby Worker Switching</h4> <ul> <li>Using separate processes for tasks incur high overhead for initialization and cleanup of the GPU environment</li> <li>Allowing multiple tasks to share one CUDA environment still incurs overhead for cleanup of the environment</li> <li>Thus, the authors propose an active-standby worker switching mechanism. <ul> <li>Each worker is a separate process with its own CUDA context. There will be a fixed number of workers, only one of which is active at anytime.</li> <li>All workers initialize its own CUDA context on startup.</li> <li>If the current active worker is stopped, it needs to clean up and free GPU memory. However, since the memory is managed by the memory daemon, cleanup consists of releasing the memory pointers.</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="mlsys"/><category term="paper-reading"/><category term="mlsys"/><category term="machine-learning"/><summary type="html"><![CDATA[Notes about PipeSwitch]]></summary></entry><entry><title type="html">[P] Rotary Positional Embedding</title><link href="https://rickysrcheng.github.io/blog/2023/positionalencoding/" rel="alternate" type="text/html" title="[P] Rotary Positional Embedding"/><published>2023-10-20T21:30:00+00:00</published><updated>2023-10-20T21:30:00+00:00</updated><id>https://rickysrcheng.github.io/blog/2023/positionalencoding</id><content type="html" xml:base="https://rickysrcheng.github.io/blog/2023/positionalencoding/"><![CDATA[<h2 id="motivation">Motivation</h2> <ul> <li>Position and order in sequential data are significant in understanding the data itself <ul> <li>For example, a common sentence structure in English is “subject - verb - object”. So the order in which the nouns appear in relation to the verb is very important.</li> <li>Another example is the position and order of adjectives in relation to the noun it is describing</li> </ul> </li> <li>Past architectures, such as LSTMs and RNNs, implicitly encode positional data by continuously computing and passing along its hidden states to the next state</li> <li>Transformer models computes all the data in parallel with no mechanism that implicitly injects positional information to any data</li> <li>Thus, positional information must be applied externally</li> </ul> <h2 id="background-materials">Background Materials</h2> <h3 id="absolute-position-encoding">Absolute Position Encoding</h3> <ul> <li> <p>The original Transformer paper by <a href="https://arxiv.org/abs/1706.03762">Vaswani et al</a> employed absolute position encoding that is added to the vector after the embedding layer using the following formula</p> <p>\begin{align} PE_{(pos)} = \begin{cases} PE_{(pos, 2i)}&amp;=\sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)\\\<br/> PE_{(pos, 2i+1)}&amp;=\cos\left(\frac{pos}{10000^{2i/d_{model}}}\right) \end{cases} \end{align}</p> <p>where \(2i\) is the \(2i^{th}\) dimension, \(pos\) denote the position of the vector, and \(d_{model}\) is the size of embedding vector. Note that even dimensions use the sine function and odd dimensions use the cosine function.</p> </li> <li>Each dimension corresponds to a sinusoid with wavelengths ranging from \(2\pi\) to \(10000\cdot 2 \pi\). Thus, in the typical usecase, it is improbable that two tokens would share the same positional encoding.</li> <li>The position encoding can also be learned, but using a function may allow the model to extrapolate positional information for sequences with lengths longer than any lengths that the model was trained on.</li> </ul> <h3 id="complex-numbers">Complex Numbers</h3> <p>The underlying principles of RoPE relies on complex algebra. To be honest, I felt a little embarrassed because I had absolutely no understanding of the paper upon the first few reads until I went back to refresh my complex numbers.</p> <ul> <li>Complex numbers take the form of \(a + bi\), where \(a\) and \(b\) are real numbers and \(i=\sqrt{-1}\) is the imaginary unit.</li> <li> <p>We can also represent a complex number in polar form:</p> \[z = a + bi = r(\cos(\theta) + i\sin(\theta)) = re^{i\theta}\] <p>where \(r= \sqrt{a^2 + b^2}\) is called the radial component and \(\theta\) is called the angular component.</p> <p>The latter equality is called Euler’s formula: \(e^{i\theta} = \cos(\theta) + i\sin(\theta)\)</p> </li> <li> <p>Multiplications of two complex numbers works the same as algebraic multiplication using distributive property</p> \[(a + bi)(c + di) = (ac - bd) + (ad + bc)i\] <p>We can also convert the above to matrix form:</p> \[\begin{bmatrix} c &amp; -d\\ d &amp; c \end{bmatrix} \begin{bmatrix} a \\ b \end{bmatrix} = \begin{bmatrix} ac - bd \\ ad + bc \end{bmatrix}\] <p>Geometrically speaking, multiplication in complex domain can be thought of as an affine transformation consisting of a scale transformation and a rotation transformation. This can be seen when we replace \(c+ di\) with its polar form \(r(cos(\theta) + i\sin(\theta))\). The matrix form will look like:</p> \[r\cdot \begin{bmatrix} \cos(\theta) &amp; -\sin(\theta)\\ \sin(\theta) &amp; \cos(\theta) \end{bmatrix} \begin{bmatrix} a \\ b \end{bmatrix} = r\cdot \begin{bmatrix} a\cos(\theta) - b\sin(\theta) \\ a\sin(\theta) + b\cos(\theta) \end{bmatrix}\] <p>The above can be interpreted as scaling the complex number \(a + bi\) by \(r\) and then rotating it counter-clockwise by \(\theta\). Note that the matrix is a <a href="https://en.wikipedia.org/wiki/Rotation_matrix">standard rotation matrix</a> in Euclidean space.</p> </li> <li> <p>The inner product for the complex case is called the <em>Hermitian inner product.</em> Given \(u, v \in \mathbb{C}^n\), it is defined as:</p> \[\langle u, v \rangle = \sum_{i=1}^{n}u_i^*v_i\] <p>where \(u^*\) is the complex conjugate of \(u\).</p> </li> </ul> <h2 id="rotary-positional-embedding">Rotary Positional Embedding</h2> <ul> <li>Rotary Positional Embedding (RoPE) is a positional embedding technique proposed by <a href="https://arxiv.org/abs/2104.09864">Jianlin Su et al</a>.</li> <li> <p>The main idea behind RoPE was to find a way to encode absolute positional information into a vector whilst at the same time use relative positional information during self-attention. In other words given vectors \(x_m, x_n \in \mathbb{R}^d\), query and key functions \(f_k(\cdot), f_q(\cdot)\), and positional information \(m, n\), we want to find a function \(g(\cdot)\) that satisfies the following:</p> <p>\begin{equation} \label{eq:relativeposition} \left&lt; f_q(\mathbf{x_m}, m), f_k(\mathbf{x_n}, n)\right&gt; = g(\mathbf{x_m}, \mathbf{x_n}, m-n) \end{equation}</p> </li> <li> <p>The solution that RoPE introduces is to map the vector \(\mathbf{x}_m \in \mathbb{R}^d\) into \(d/2\) subspace in the complex domain \(\mathbb{C}^{d/2}\) by considering consecutive elements in \(\mathbf{x}_m\) as one complex number. As in</p> \[(x_1, x_2, \dots, x_{d-1}, x_d) \rightarrow (x_1 + i x_2 , \dots, x_{d-1} + i x_d )\] <p>This necessitates that the dimension of the original vector be even, which can be done by adding an additional dimension if the vector dimension is odd.</p> <p>We then inject positional information via rotation, which looks something like this:</p> \[f^{RoPE}(\mathbf{x}_m, m)_{j} = (x^{(2j)} + ix^{(2j+1)})e^{mi\theta_j}\] <p>Or in rotational matrix form:</p> \[f^{RoPE}(\mathbf{x}_m, m)_{j} = \begin{bmatrix} \cos(m\theta_j) &amp; -\sin(m\theta_j)\\ \sin(m\theta_j) &amp; \cos(m\theta_j) \end{bmatrix} \begin{bmatrix} x^{(2j)} \\ x^{(2j+1)} \end{bmatrix}\] <p>\(\theta_j\) is the sinusoidal wave for dimension \(j\) and is defined to be \(\theta_j = 10000^{-2j/d}\). As a shorthand, we can let \(R_{\theta_j, m}\) be the rotation matrix with the parameters \(\theta_j\) and \(m\).</p> <ul> <li> <p>The more general form of RoPE is given as</p> \[f^{RoPE}(\mathbf{x_m}, m) = R_{\Theta, m}\mathbf{x_m}\] <p>with the rotational matrix \(R_{\Theta, m}\) defined as:</p> \[R_{\Theta, m}= \begin{bmatrix} R_{\theta_0, m} &amp; &amp; &amp;\\ &amp; R_{\theta_1, m} &amp; \\ &amp; &amp; \ddots &amp;\\ &amp; &amp; &amp; R_{\theta_{d/2-1}, m}\\ \end{bmatrix}\] </li> </ul> </li> <li> <p>The above formulation of RoPE can then satisfy \eqref{eq:relativeposition}. For simplicity, we show the 2D case where \(\mathbf{x_m}, \mathbf{x_n} \in \mathbb{R}^2\):</p> \[\begin{align} \left&lt; f_q(\mathbf{x_m}, m), f_k(\mathbf{x_n}, n)\right&gt;_\mathbb{R} &amp;= \text{Re}(\left&lt; f^{RoPE}_q(\mathbf{x_m}, m), f^{RoPE}_k(\mathbf{x_n}, n)\right&gt;_\mathbb{C})\\ &amp;= \text{Re}(\hat{x}^*_m e^{-im\theta} \hat{x}_ne^{in\theta})\\ &amp;= \text{Re}\Big(\big((x^{(1)}_mx^{(1)}_n + x^{(2)}_mx^{(2)}_n) + i(-x^{(1)}_mx^{(2)}_n + x^{(2)}_mx^{(1)}_n)\big) e^{i(n-m)\theta}\Big)\\ &amp;= \cos\big((n-m)\theta\big) (x^{(1)}_mx^{(1)}_n + x^{(2)}_mx^{(2)}_n) \\ &amp;\quad\quad - \sin\big((n-m)\theta\big)(-x^{(1)}_mx^{(2)}_n + x^{(2)}_mx^{(1)}_n)\\ &amp;= \mathbf{x}^\intercal_\mathbf{m} R_{\theta, n-m} \mathbf{x_n}\\ &amp;= g(\mathbf{x_m}, \mathbf{x_n}, n-m) \end{align}\] <ul> <li>A big part of my confusion stems from the fact that this equivalence is given as true: \(\mathbf{q}\mathbf{k} = \text{Re}(\mathbf{q}^* \mathbf{k})\), which it is but I didn’t know how it got there since the vectors \(\mathbf{q}, \mathbf{k}\) are reused and the authors are relying on the readers to implicitly know if the vector is \(\mathbb{R}^d\) or \(\mathbb{C}^{d/2}\). If one isn’t being careful and assumed both vectors are in \(\mathbb{R}^d\), then one can observe that the complex conjugate of a real vector is itself, so \(qk = q^*k\), which means \(qk = \text{Re}(q^*k)\), but this isn’t the point being made in this paper.</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="machine-learning"/><category term="nlp"/><category term="paper-reading"/><category term="nlp"/><category term="transformer"/><category term="machine-learning"/><summary type="html"><![CDATA[Notes about RoPE]]></summary></entry><entry><title type="html">Hello World</title><link href="https://rickysrcheng.github.io/blog/2023/first-post/" rel="alternate" type="text/html" title="Hello World"/><published>2023-10-20T17:30:00+00:00</published><updated>2023-10-20T17:30:00+00:00</updated><id>https://rickysrcheng.github.io/blog/2023/first%20post</id><content type="html" xml:base="https://rickysrcheng.github.io/blog/2023/first-post/"><![CDATA[<p>Hello! Welcome to my blog/website. I really like the idea of maintaining a personal space where I can just log my thoughts and learnings in a single place that’s convenient for me to revisit. However, I just never really gotten around to committing to one. I initially started using Github pages with the Hugo PaperMod theme. While the theme is good, I felt that it was missing a few features so I didn’t really stick with it. And then I found the al-folio theme on Jekyll and liked it enough to restart my blog.</p>]]></content><author><name></name></author><category term="blog"/><summary type="html"><![CDATA[first post]]></summary></entry></feed>