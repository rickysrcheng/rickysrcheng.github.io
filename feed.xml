<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://rickysrcheng.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rickysrcheng.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-04-26T21:02:54+00:00</updated><id>https://rickysrcheng.github.io/feed.xml</id><title type="html">Ricky Cheng</title><subtitle></subtitle><entry><title type="html">[P] The Reversible Residual Network</title><link href="https://rickysrcheng.github.io/blog/2024/revnet/" rel="alternate" type="text/html" title="[P] The Reversible Residual Network"/><published>2024-04-26T11:37:00+00:00</published><updated>2024-04-26T11:37:00+00:00</updated><id>https://rickysrcheng.github.io/blog/2024/revnet</id><content type="html" xml:base="https://rickysrcheng.github.io/blog/2024/revnet/"><![CDATA[<h2 id="motivation">Motivation</h2> <ul> <li>ResNet advanced the state of deep learning and allowed for much deeper and wider deep neural networks to be trained</li> <li>However, as networks get bigger, memory consumption increases due to the need to store layer activations during forward pass for gradient updates in backpropagation</li> <li>May be beneficial to trade compute time for memory space, especially in systems where GPU memory is limited or complex to manage (such as multi-GPU distributed training)</li> </ul> <h2 id="background-materials">Background Materials</h2> <h4 id="backpropagation-and-saving-activation-values">Backpropagation and Saving Activation Values</h4> <ul> <li>Backpropagation is a technique used to compute gradient of a cost function with respect to a network node. Essentially, backpropagation <em>propagates</em> the error gradient from the cost function back into the gradient of the network. These gradients are then used by an optimization algorithm, such as stochastic gradient descent, to update the network parameters. The details of backpropagation can be found in numerous online sources, thus I won’t go into details here.</li> <li>The main issue the paper tries to tackle is that gradients of activation values require either the activation values or pre-activation values to compute <ul> <li>The popular ReLU activation function \(g(x) = \text{max}(0, x)\) has the derivative \(g'(x) = \begin{cases} 0 &amp; x &lt; 0 \\ 1 &amp; x &gt; 0\end{cases}\). This requires knowledge of the pre-activation values \(x\).</li> </ul> </li> <li>A technique used to reduce the amount of memory used to save activation values is called <em>gradient checkpointing</em>, which saves activation values every few layers and recomputes activation values for layers that don’t have it saved. This trades memory for compute time.</li> </ul> <h4 id="resnet">ResNet</h4> <ul> <li>Introduced in 2016 by <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf">He et al</a>, ResNet is an architecture that uses residual learning blocks that made it possible to train deeper networks.</li> </ul> <div class="row mt-3 justify-content-center"> <div class="col-sm mt-3 mt-md-0" style="max-width:500px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/revnet-resnet-480.webp 480w,/assets/img/revnet-resnet-800.webp 800w,/assets/img/revnet-resnet-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/revnet-resnet.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>ResNets are built out of residual blocks, which takes the form of</p> \[y = x + \mathcal{F}(x)\] <p>where \(\mathcal{F}(x)\) is called a residual function, typically a small and shallow neural network. The skip connection allows information to flow freely through the network, which alleviates the vanishing gradient problem.</p> </li> </ul> <h4 id="reversible-architectures">Reversible Architectures</h4> <ul> <li>Reversible architectures are models that can return the input given only the model itself and the outputs, i.e. \(y = f(x)\) and \(x = f^{-1}(y)\).</li> <li>The architecture that inspired the authors is from <a href="https://arxiv.org/pdf/1410.8516.pdf">non-linear independent components estimation (NICE)</a>. The</li> <li> <p>Given \(x\), we partition \(x\) into \((x_1, x_2)\) along some dimension. Then we get the following forward mapping:</p> \[\begin{align*} y_1 &amp; = x_1 \\ y_2 &amp; = x_2 + \mathcal{F}(x_1) \end{align*}\] <p>We can get the input \(x_1, x_2\) back if we have the outputs \(y_1, y_2\) using the following inverse mapping:</p> \[\begin{align*} x_1 &amp; = y_1 \\ x_2 &amp; = y_2 - \mathcal{F}(y_1) \end{align*}\] </li> </ul> <h2 id="reversible-resnet">Reversible ResNet</h2> <ul> <li>In <a href="https://arxiv.org/abs/1707.04585">RevNet</a>, the authors proposed incorporating a reversible architecture into ResNet. The reversibility allows each block’s pre-activations to be calculated directly from the next layer’s activations, thereby saving memory.</li> </ul> <h4 id="architecture">Architecture</h4> <div class="row mt-3 justify-content-center"> <div class="col-sm mt-3 mt-md-0" style="max-width:550px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/revnet-arch-480.webp 480w,/assets/img/revnet-arch-800.webp 800w,/assets/img/revnet-arch-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/revnet-arch.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li> <p>RevNet is built out of reversible residual blocks. Given an input \(x\), we first partition \(x\) into \((x_1, x_2)\) along a dimension (the paper chose to partition along the channel). The resulting output is given as:</p> \[\begin{align*} y_1 &amp; = x_1 + \mathcal{F}(x_2)\\ y_2 &amp; = x_2 + \mathcal{G}(y_1) \end{align*}\] <p>where the functions \(\mathcal{F}(\cdot)\) and \(\mathcal{G}(\cdot)\) are analogous to the residual functions found in ResNet.</p> <p>The inverse mapping, similar to NICE, is given by:</p> \[\begin{align*} x_1 &amp; = y_1 - \mathcal{F}(x_2)\\ x_2 &amp; = y_2 - \mathcal{G}(y_1) \end{align*}\] </li> <li> <p>Unlike residual blocks, reversible blocks must have a stride of 1. Otherwise information is discarded, which makes the layer irreversible. Activations still needs to be explicitly stored for non-reversible layers.</p> </li> </ul> <h4 id="backpropagation">Backpropagation</h4> <ul> <li>The backpropagation for RevNet is given below. The paper used \(\bar{v}_i\) to denote the total derivative of node \(v_i\) with respect to the cost function \(\mathcal{C}\), \(\bar{v}_i = d\mathcal{C}/d v_i\). <pre><code class="language-pseudocode">\begin{algorithm}
\caption{RevNet Backprop}
\begin{algorithmic}
\PROCEDURE{RevNetBackprop}{$$ (y_1, y_2), (\bar{y}_1, \bar{y}_2) $$}
  \STATE \COMMENT{Retrieve input activation}
  \STATE $$x_2 \gets y_2 - \mathcal{G}(y_1)$$
  \STATE $$x_1 \gets y_1 - \mathcal{F}(x_2)$$
  \STATE \COMMENT{Calculate activation gradient}
  \STATE $$\bar{x}_1 \gets \bar{y}_1 + (\frac{\partial \mathcal{G}}{\partial y_1})^\intercal\bar{y}_2$$
  \STATE $$\bar{x}_2 \gets \bar{y}_2 + (\frac{\partial \mathcal{F}}{\partial y_2})^\intercal \bar{x}_1$$
  \STATE \COMMENT{Calculate parameter gradient}
  \STATE $$\bar{\theta}_\mathcal{F} \gets (\frac{\partial \mathcal{F}}{\partial \theta_{\mathcal{F}}})^\intercal \bar{x}_1$$
  \STATE $$\bar{\theta}_\mathcal{G} \gets (\frac{\partial \mathcal{G}}{\partial \theta_{\mathcal{G}}})^\intercal \bar{y}_2$$
  \RETURN $$(x_1, x_2), (\bar{x}_1, \bar{x}_2), (\bar{\theta}_\mathcal{F}, \bar{\theta}_\mathcal{G})$$
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre> </li> <li>In contrast, the algorithm of ResNet is given below. Note that it requires \(x\) be given as input: <pre><code class="language-pseudocode">\begin{algorithm}
\caption{RevNet Backprop}
\begin{algorithmic}
\PROCEDURE{RevNetBackprop}{$$ x, y, \bar{y} $$}
  \STATE $$\bar{x} \gets \bar{y} + (\frac{\partial F}{\partial x})^\intercal \bar{y}$$
  \STATE $$\bar{\theta}_\mathcal{F} \gets (\frac{\partial F}{\partial \theta_\mathcal{F}})^\intercal \bar{y}$$
  \RETURN $$\bar{x}, \bar{\theta}_\mathcal{F}$$
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre> </li> </ul> <h2 id="experiments">Experiments</h2> <ul> <li>Since RevNet blocks have around twice the computation depth of a ResNet block. Thus, the RevNet models have approximately half the number of blocks and twice the number of channels than its ResNet counterpart.</li> <li>The experiments showed that RevNet performs similarly to ResNet in all of its classification tests and also matches the training dynamics.</li> <li>The paper doesn’t cite a figure for how much memory saving was achieved. However, the authors did claim that, using the same GPU resources, RevNet was able to train on a mini-batch size of 128 images, whereas its ResNet counterpart was only able to train on mini-batch size of 32 images.</li> </ul> <h2 id="references">References</h2> <ul> <li>The Reversible Residual Network: Backpropagation Without Storing Activations, Gomez et al., NeurIPS 2017</li> <li>NICE: Non-linear Independent Components Estimation, Dinh et al., ICLR 2015</li> <li>Deep Residual Learning for Image Recognition, He et al., CVPR 2016</li> </ul>]]></content><author><name></name></author><category term="machine-learning"/><category term="paper-reading"/><category term="machine-learning"/><category term="backpropagation"/><category term="reversible-architecture"/><summary type="html"><![CDATA[Notes about RevNet]]></summary></entry><entry><title type="html">[P] PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications</title><link href="https://rickysrcheng.github.io/blog/2024/pipeswitch/" rel="alternate" type="text/html" title="[P] PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications"/><published>2024-03-21T21:30:00+00:00</published><updated>2024-03-21T21:30:00+00:00</updated><id>https://rickysrcheng.github.io/blog/2024/pipeswitch</id><content type="html" xml:base="https://rickysrcheng.github.io/blog/2024/pipeswitch/"><![CDATA[<h2 id="motivation">Motivation</h2> <ul> <li>Deep learning tasks have two primary workloads: training and inference. <ul> <li>Training workloads are throughput intensive but are more flexible in latency, since it typically takes a while to finish.</li> <li>Inference workloads are latency sensitive and have uncertain workloads.</li> </ul> </li> <li>Typical practice is to provision separate GPU clusters for training and inference, which leads to inefficiencies <ul> <li>Inference clusters are often over-provisioned to satisfy peak workload</li> <li>Training workloads cannot use inference clusters when inference load is low</li> </ul> </li> <li>Ideally, unify resources and use one cluster for both tasks via time-sharing <ul> <li>Problem: overhead for context switching is high for GPUs, which impacts inference latency requirements</li> </ul> </li> </ul> <h2 id="pipeswitch">PipeSwitch</h2> <ul> <li>PipeSwitch is a system proposed by <a href="https://www.usenix.org/conference/osdi20/presentation/bai">Zhihao Bai et al</a> to allow efficient time-sharing of a GPU</li> </ul> <div class="row mt-3 justify-content-center"> <div class="col-sm mt-3 mt-md-0" style="max-width:400px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pipeswitch-arch-480.webp 480w,/assets/img/pipeswitch-arch-800.webp 800w,/assets/img/pipeswitch-arch-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/pipeswitch-arch.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> PipeSwitch Architecture </div> <ul> <li>PipeSwitch consists of the following subsystems: <ul> <li>Controller: Responsible for handling client requests, scheduling workers to the GPU, and directing the memory daemon to allocate and transfer model weights</li> <li>Memory Daemon: Allocates GPU memory to the active worker and transfers model weights between host and GPU memory</li> <li>Active/Standby workers: Active workers are workers currently executing a task in the GPU. Standby workers are workers are workers who may be idle, initializing a new task, or cleaning up a previous task</li> </ul> </li> </ul> <h2 id="design-mechanisms">Design Mechanisms</h2> <p>PipeSwitch introduces three main mechanisms that work together to reduce context switching overhead</p> <h4 id="pipelined-model-transmisson">Pipelined Model Transmisson</h4> <div class="row mt-3 justify-content-center"> <div class="col-sm mt-3 mt-md-0" style="max-width:450px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pipeswitch-pipeline-480.webp 480w,/assets/img/pipeswitch-pipeline-800.webp 800w,/assets/img/pipeswitch-pipeline-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/pipeswitch-pipeline.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Pipelined model transmission can shorten latency and have better utilization of GPU resources </div> <ul> <li>Deep learning models employ a layered architecture which means that not all layer weights need to be loaded into memory to begin execution. This observation means that we can pipeline the model execution and data transmission <ul> <li>However, GPU cores executes calculations much faster than memory operations. Thus, need to consider the granularity. <ul> <li>Whole model granularity is the same as loading the whole model into memory before execution.</li> <li>Per layer granularity operates on one layer at a time. However, this may incur significant overhead due to PCIe costs and synchronization costs.</li> </ul> </li> </ul> </li> <li>To this end, the authors propose <em>model-aware grouping</em> to find the optimal grouping to transmit and execute. The algorithm considers both the layer and number of layers to group together. The algorithm and proof, which I will not go into, is provided in the paper for those who are interested.</li> </ul> <h4 id="unified-memory-management">Unified Memory Management</h4> <ul> <li> <p>Since DL tasks require GPU memory, the authors proposed a unified memory management system to reduce overhead from allocating and transmitting the models. This is because the memory footprint of the model parameters remain unchanged during task execution. Neither forward pass nor backpropagation will change the model structure. In addition, while intermediate results are needed during training, they are produced and consumed in a structured and predictable manner. Thus, a general purpose memory management system, like what CUDA provides, is too heavyweight.</p> </li> <li> <p>The unified memory management has four mechanisms that helps to reduce memory overhead:</p> <ol> <li><strong>Minimize Memory Allocation Overhead</strong>: Essentially, the memory daemon sits on top of CUDA and obtains GPU memory on startup. This eliminates the need for each task worker to allocate its own GPU memory. Instead, the daemon, having already obtained GPU memory, will pass a pointer to the worker, saving overhead. This also allows the daemon to guarantee memory isolation between workers.</li> <li><strong>Minimize memory footprint and avoid extra memory copies</strong>: It may be a case that the same model is needed for multiple tasks. Having each task have a duplicate copy wastes memory spaces. However, having a separate process to save models in host memory would incur memory overhead from transferring the model to the task to transmit to the GPU. The memory daemon solves both as it keeps one copy of each model in host memory. Since it manages both host and GPU memory, it can also transmit the model to GPU directly.</li> <li><strong>Minimize IPC Overhead</strong>: I’m not too familiar with GPU IPCs and the associated overheads; though I still tried my best in understanding this mechanism. <br/> Since the model transmission is pipelined, synchronization needs to occur between the memory daemon and the worker. However, using GPU IPC is expensive. However, the authors observed a property that memory allocation for a neural network model is <em>deterministic</em>. So, given the same model and GPU memory region, the memory pointers for each pipeline group would be the same as long as the allocation order is the same between the memory daemon and the worker. Thus, we can take advantage of this and use CPU IPCs, which are cheap, in place of GPU IPCs to signal which pipeline group is transmitted.</li> <li><strong>Pin Memory</strong>: GPUs require a page to be pinned in host memory for memory transmission. If no page is pinned in host memory, a temporary page is pinned for transmission. The authors thus pin the pages of the memory daemon to avoid such overhead.</li> </ol> </li> </ul> <h4 id="active-standby-worker-switching">Active-Standby Worker Switching</h4> <ul> <li>Using separate processes for tasks incur high overhead for initialization and cleanup of the GPU environment</li> <li>Allowing multiple tasks to share one CUDA environment still incurs overhead for cleanup of the environment</li> <li>Thus, the authors propose an active-standby worker switching mechanism. <ul> <li>Each worker is a separate process with its own CUDA context. There will be a fixed number of workers, only one of which is active at anytime.</li> <li>All workers initialize its own CUDA context on startup.</li> <li>If the current active worker is stopped, it needs to clean up and free GPU memory. However, since the memory is managed by the memory daemon, cleanup consists of releasing the memory pointers.</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="mlsys"/><category term="paper-reading"/><category term="mlsys"/><category term="machine-learning"/><summary type="html"><![CDATA[Notes about PipeSwitch]]></summary></entry><entry><title type="html">[P] Rotary Positional Embedding</title><link href="https://rickysrcheng.github.io/blog/2023/positionalencoding/" rel="alternate" type="text/html" title="[P] Rotary Positional Embedding"/><published>2023-10-20T21:30:00+00:00</published><updated>2023-10-20T21:30:00+00:00</updated><id>https://rickysrcheng.github.io/blog/2023/positionalencoding</id><content type="html" xml:base="https://rickysrcheng.github.io/blog/2023/positionalencoding/"><![CDATA[<h2 id="motivation">Motivation</h2> <ul> <li>Position and order in sequential data are significant in understanding the data itself <ul> <li>For example, a common sentence structure in English is “subject - verb - object”. So the order in which the nouns appear in relation to the verb is very important.</li> <li>Another example is the position and order of adjectives in relation to the noun it is describing</li> </ul> </li> <li>Past architectures, such as LSTMs and RNNs, implicitly encode positional data by continuously computing and passing along its hidden states to the next state</li> <li>Transformer models computes all the data in parallel with no mechanism that implicitly injects positional information to any data</li> <li>Thus, positional information must be applied externally</li> </ul> <h2 id="background-materials">Background Materials</h2> <h3 id="absolute-position-encoding">Absolute Position Encoding</h3> <ul> <li> <p>The original Transformer paper by <a href="https://arxiv.org/abs/1706.03762">Vaswani et al</a> employed absolute position encoding that is added to the vector after the embedding layer using the following formula</p> <p>\begin{align} PE_{(pos)} = \begin{cases} PE_{(pos, 2i)}&amp;=\sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)\\\<br/> PE_{(pos, 2i+1)}&amp;=\cos\left(\frac{pos}{10000^{2i/d_{model}}}\right) \end{cases} \end{align}</p> <p>where \(2i\) is the \(2i^{th}\) dimension, \(pos\) denote the position of the vector, and \(d_{model}\) is the size of embedding vector. Note that even dimensions use the sine function and odd dimensions use the cosine function.</p> </li> <li>Each dimension corresponds to a sinusoid with wavelengths ranging from \(2\pi\) to \(10000\cdot 2 \pi\). Thus, in the typical usecase, it is improbable that two tokens would share the same positional encoding.</li> <li>The position encoding can also be learned, but using a function may allow the model to extrapolate positional information for sequences with lengths longer than any lengths that the model was trained on.</li> </ul> <h3 id="complex-numbers">Complex Numbers</h3> <p>The underlying principles of RoPE relies on complex algebra. To be honest, I felt a little embarrassed because I had absolutely no understanding of the paper upon the first few reads until I went back to refresh my complex numbers.</p> <ul> <li>Complex numbers take the form of \(a + bi\), where \(a\) and \(b\) are real numbers and \(i=\sqrt{-1}\) is the imaginary unit.</li> <li> <p>We can also represent a complex number in polar form:</p> \[z = a + bi = r(\cos(\theta) + i\sin(\theta)) = re^{i\theta}\] <p>where \(r= \sqrt{a^2 + b^2}\) is called the radial component and \(\theta\) is called the angular component.</p> <p>The latter equality is called Euler’s formula: \(e^{i\theta} = \cos(\theta) + i\sin(\theta)\)</p> </li> <li> <p>Multiplications of two complex numbers works the same as algebraic multiplication using distributive property</p> \[(a + bi)(c + di) = (ac - bd) + (ad + bc)i\] <p>We can also convert the above to matrix form:</p> \[\begin{bmatrix} c &amp; -d\\ d &amp; c \end{bmatrix} \begin{bmatrix} a \\ b \end{bmatrix} = \begin{bmatrix} ac - bd \\ ad + bc \end{bmatrix}\] <p>Geometrically speaking, multiplication in complex domain can be thought of as an affine transformation consisting of a scale transformation and a rotation transformation. This can be seen when we replace \(c+ di\) with its polar form \(r(cos(\theta) + i\sin(\theta))\). The matrix form will look like:</p> \[r\cdot \begin{bmatrix} \cos(\theta) &amp; -\sin(\theta)\\ \sin(\theta) &amp; \cos(\theta) \end{bmatrix} \begin{bmatrix} a \\ b \end{bmatrix} = r\cdot \begin{bmatrix} a\cos(\theta) - b\sin(\theta) \\ a\sin(\theta) + b\cos(\theta) \end{bmatrix}\] <p>The above can be interpreted as scaling the complex number \(a + bi\) by \(r\) and then rotating it counter-clockwise by \(\theta\). Note that the matrix is a <a href="https://en.wikipedia.org/wiki/Rotation_matrix">standard rotation matrix</a> in Euclidean space.</p> </li> <li> <p>The inner product for the complex case is called the <em>Hermitian inner product.</em> Given \(u, v \in \mathbb{C}^n\), it is defined as:</p> \[\langle u, v \rangle = \sum_{i=1}^{n}u_i^*v_i\] <p>where \(u^*\) is the complex conjugate of \(u\).</p> </li> </ul> <h2 id="rotary-positional-embedding">Rotary Positional Embedding</h2> <ul> <li>Rotary Positional Embedding (RoPE) is a positional embedding technique proposed by <a href="https://arxiv.org/abs/2104.09864">Jianlin Su et al</a>.</li> <li> <p>The main idea behind RoPE was to find a way to encode absolute positional information into a vector whilst at the same time use relative positional information during self-attention. In other words given vectors \(x_m, x_n \in \mathbb{R}^d\), query and key functions \(f_k(\cdot), f_q(\cdot)\), and positional information \(m, n\), we want to find a function \(g(\cdot)\) that satisfies the following:</p> <p>\begin{equation} \label{eq:relativeposition} \left&lt; f_q(\mathbf{x_m}, m), f_k(\mathbf{x_n}, n)\right&gt; = g(\mathbf{x_m}, \mathbf{x_n}, m-n) \end{equation}</p> </li> <li> <p>The solution that RoPE introduces is to map the vector \(\mathbf{x}_m \in \mathbb{R}^d\) into \(d/2\) subspace in the complex domain \(\mathbb{C}^{d/2}\) by considering consecutive elements in \(\mathbf{x}_m\) as one complex number. As in</p> \[(x_1, x_2, \dots, x_{d-1}, x_d) \rightarrow (x_1 + i x_2 , \dots, x_{d-1} + i x_d )\] <p>This necessitates that the dimension of the original vector be even, which can be done by adding an additional dimension if the vector dimension is odd.</p> <p>We then inject positional information via rotation, which looks something like this:</p> \[f^{RoPE}(\mathbf{x}_m, m)_{j} = (x^{(2j)} + ix^{(2j+1)})e^{mi\theta_j}\] <p>Or in rotational matrix form:</p> \[f^{RoPE}(\mathbf{x}_m, m)_{j} = \begin{bmatrix} \cos(m\theta_j) &amp; -\sin(m\theta_j)\\ \sin(m\theta_j) &amp; \cos(m\theta_j) \end{bmatrix} \begin{bmatrix} x^{(2j)} \\ x^{(2j+1)} \end{bmatrix}\] <p>\(\theta_j\) is the sinusoidal wave for dimension \(j\) and is defined to be \(\theta_j = 10000^{-2j/d}\). As a shorthand, we can let \(R_{\theta_j, m}\) be the rotation matrix with the parameters \(\theta_j\) and \(m\).</p> <ul> <li> <p>The more general form of RoPE is given as</p> \[f^{RoPE}(\mathbf{x_m}, m) = R_{\Theta, m}\mathbf{x_m}\] <p>with the rotational matrix \(R_{\Theta, m}\) defined as:</p> \[R_{\Theta, m}= \begin{bmatrix} R_{\theta_0, m} &amp; &amp; &amp;\\ &amp; R_{\theta_1, m} &amp; \\ &amp; &amp; \ddots &amp;\\ &amp; &amp; &amp; R_{\theta_{d/2-1}, m}\\ \end{bmatrix}\] </li> </ul> </li> <li> <p>The above formulation of RoPE can then satisfy \eqref{eq:relativeposition}. For simplicity, we show the 2D case where \(\mathbf{x_m}, \mathbf{x_n} \in \mathbb{R}^2\):</p> \[\begin{align} \left&lt; f_q(\mathbf{x_m}, m), f_k(\mathbf{x_n}, n)\right&gt;_\mathbb{R} &amp;= \text{Re}(\left&lt; f^{RoPE}_q(\mathbf{x_m}, m), f^{RoPE}_k(\mathbf{x_n}, n)\right&gt;_\mathbb{C})\\ &amp;= \text{Re}(\hat{x}^*_m e^{-im\theta} \hat{x}_ne^{in\theta})\\ &amp;= \text{Re}\Big(\big((x^{(1)}_mx^{(1)}_n + x^{(2)}_mx^{(2)}_n) + i(-x^{(1)}_mx^{(2)}_n + x^{(2)}_mx^{(1)}_n)\big) e^{i(n-m)\theta}\Big)\\ &amp;= \cos\big((n-m)\theta\big) (x^{(1)}_mx^{(1)}_n + x^{(2)}_mx^{(2)}_n) \\ &amp;\quad\quad - \sin\big((n-m)\theta\big)(-x^{(1)}_mx^{(2)}_n + x^{(2)}_mx^{(1)}_n)\\ &amp;= \mathbf{x}^\intercal_\mathbf{m} R_{\theta, n-m} \mathbf{x_n}\\ &amp;= g(\mathbf{x_m}, \mathbf{x_n}, n-m) \end{align}\] <ul> <li>A big part of my confusion stems from the fact that this equivalence is given as true: \(\mathbf{q}\mathbf{k} = \text{Re}(\mathbf{q}^* \mathbf{k})\), which it is but I didn’t know how it got there since the vectors \(\mathbf{q}, \mathbf{k}\) are reused and the authors are relying on the readers to implicitly know if the vector is \(\mathbb{R}^d\) or \(\mathbb{C}^{d/2}\). If one isn’t being careful and assumed both vectors are in \(\mathbb{R}^d\), then one can observe that the complex conjugate of a real vector is itself, so \(qk = q^*k\), which means \(qk = \text{Re}(q^*k)\), but this isn’t the point being made in this paper.</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="machine-learning"/><category term="nlp"/><category term="paper-reading"/><category term="nlp"/><category term="transformer"/><category term="machine-learning"/><summary type="html"><![CDATA[Notes about RoPE]]></summary></entry><entry><title type="html">Hello World</title><link href="https://rickysrcheng.github.io/blog/2023/first-post/" rel="alternate" type="text/html" title="Hello World"/><published>2023-10-20T17:30:00+00:00</published><updated>2023-10-20T17:30:00+00:00</updated><id>https://rickysrcheng.github.io/blog/2023/first%20post</id><content type="html" xml:base="https://rickysrcheng.github.io/blog/2023/first-post/"><![CDATA[<p>Hello! Welcome to my blog/website. I really like the idea of maintaining a personal space where I can just log my thoughts and learnings in a single place that’s convenient for me to revisit. However, I just never really gotten around to committing to one. I initially started using Github pages with the Hugo PaperMod theme. While the theme is good, I felt that it was missing a few features so I didn’t really stick with it. And then I found the al-folio theme on Jekyll and liked it enough to restart my blog.</p>]]></content><author><name></name></author><category term="blog"/><summary type="html"><![CDATA[first post]]></summary></entry></feed>