<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://rickysrcheng.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rickysrcheng.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-03-21T17:11:57+00:00</updated><id>https://rickysrcheng.github.io/feed.xml</id><title type="html">Ricky Cheng</title><subtitle></subtitle><entry><title type="html">Rotary Positional Embedding</title><link href="https://rickysrcheng.github.io/blog/2023/positionalencoding/" rel="alternate" type="text/html" title="Rotary Positional Embedding"/><published>2023-10-20T21:30:00+00:00</published><updated>2023-10-20T21:30:00+00:00</updated><id>https://rickysrcheng.github.io/blog/2023/positionalencoding</id><content type="html" xml:base="https://rickysrcheng.github.io/blog/2023/positionalencoding/"><![CDATA[<h2 id="motivation">Motivation</h2> <ul> <li>Position and order in sequential data are significant in understanding the data itself <ul> <li>For example, a common sentence structure in English is “subject - verb - object”. So the order in which the nouns appear in relation to the verb is very important.</li> <li>Another example is the position and order of adjectives in relation to the noun it is describing</li> </ul> </li> <li>Past architectures, such as LSTMs and RNNs, implicitly encode positional data by continuously computing and passing along its hidden states to the next state</li> <li>Transformer models computes all the data in parallel with no mechanism that implicitly injects positional information to any data</li> <li>Thus, positional information must be applied externally</li> </ul> <h2 id="absolute-position-encoding">Absolute Position Encoding</h2> <ul> <li> <p>The original Transformer paper by <a href="https://arxiv.org/abs/1706.03762">Vaswani et al</a> employed absolute position encoding that is added to the vector after the embedding layer using the following formula</p> <p>\begin{align} PE_{(pos)} = \begin{cases} PE_{(pos, 2i)}&amp;=\sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)\\\<br/> PE_{(pos, 2i+1)}&amp;=\cos\left(\frac{pos}{10000^{2i/d_{model}}}\right) \end{cases} \end{align}</p> <p>where \(2i\) is the \(2i^{th}\) dimension, \(pos\) denote the position of the vector, and \(d_{model}\) is the size of embedding vector. Note that even dimensions use the sine function and odd dimensions use the cosine function.</p> </li> <li>Each dimension corresponds to a sinusoid with wavelengths ranging from \(2\pi\) to \(10000\cdot 2 \pi\). Thus, in the typical usecase, it is improbable that two tokens would share the same positional encoding.</li> <li>The position encoding can also be learned, but using a function may allow the model to extrapolate positional information for sequences with lengths longer than any lengths that the model was trained on.</li> </ul> <h2 id="complex-numbers">Complex Numbers</h2> <p>The underlying principles of RoPE relies on complex algebra. To be honest, I felt a little embarrassed because I had absolutely no understanding of the paper upon the first few reads until I went back to refresh my complex numbers.</p> <ul> <li>Complex numbers take the form of \(a + bi\), where \(a\) and \(b\) are real numbers and \(i=\sqrt{-1}\) is the imaginary unit.</li> <li> <p>We can also represent a complex number in polar form:</p> \[z = a + bi = r(\cos(\theta) + i\sin(\theta)) = re^{i\theta}\] <p>where \(r= \sqrt{a^2 + b^2}\) is called the radial component and \(\theta\) is called the angular component.</p> <p>The latter equality is called Euler’s formula: \(e^{i\theta} = \cos(\theta) + i\sin(\theta)\)</p> </li> <li> <p>Multiplications of two complex numbers works the same as algebraic multiplication using distributive property</p> \[(a + bi)(c + di) = (ac - bd) + (ad + bc)i\] <p>We can also convert the above to matrix form:</p> \[\begin{bmatrix} c &amp; -d\\ d &amp; c \end{bmatrix} \begin{bmatrix} a \\ b \end{bmatrix} = \begin{bmatrix} ac - bd \\ ad + bc \end{bmatrix}\] <p>Geometrically speaking, multiplication in complex domain can be thought of as an affine transformation consisting of a scale transformation and a rotation transformation. This can be seen when we replace \(c+ di\) with its polar form \(r(cos(\theta) + i\sin(\theta))\). The matrix form will look like:</p> \[r\cdot \begin{bmatrix} \cos(\theta) &amp; -\sin(\theta)\\ \sin(\theta) &amp; \cos(\theta) \end{bmatrix} \begin{bmatrix} a \\ b \end{bmatrix} = r\cdot \begin{bmatrix} a\cos(\theta) - b\sin(\theta) \\ a\sin(\theta) + b\cos(\theta) \end{bmatrix}\] <p>The above can be interpreted as scaling the complex number \(a + bi\) by \(r\) and then rotating it counter-clockwise by \(\theta\). Note that the matrix is a <a href="https://en.wikipedia.org/wiki/Rotation_matrix">standard rotation matrix</a> in Euclidean space.</p> </li> <li> <p>The inner product for the complex case is called the <em>Hermitian inner product.</em> Given \(u, v \in \mathbb{C}^n\), it is defined as:</p> \[\langle u, v \rangle = \sum_{i=1}^{n}u_i^*v_i\] <p>where \(u^*\) is the complex conjugate of \(u\).</p> </li> </ul> <h2 id="rotary-positional-embedding">Rotary Positional Embedding</h2> <ul> <li>Rotary Positional Embedding (RoPE) is a positional embedding technique proposed by <a href="https://arxiv.org/abs/2104.09864">Jianlin Su et al</a>.</li> <li> <p>The main idea behind RoPE was to find a way to encode absolute positional information into a vector whilst at the same time use relative positional information during self-attention. In other words given vectors \(x_m, x_n \in \mathbb{R}^d\), query and key functions \(f_k(\cdot), f_q(\cdot)\), and positional information \(m, n\), we want to find a function \(g(\cdot)\) that satisfies the following:</p> <p>\begin{equation} \label{eq:relativeposition} \left&lt; f_q(\mathbf{x_m}, m), f_k(\mathbf{x_n}, n)\right&gt; = g(\mathbf{x_m}, \mathbf{x_n}, m-n) \end{equation}</p> </li> <li> <p>The solution that RoPE introduces is to map the vector \(\mathbf{x}_m \in \mathbb{R}^d\) into \(d/2\) subspace in the complex domain \(\mathbb{C}^{d/2}\) by considering consecutive elements in \(\mathbf{x}_m\) as one complex number. As in</p> \[(x_1, x_2, \dots, x_{d-1}, x_d) \rightarrow (x_1 + i x_2 , \dots, x_{d-1} + i x_d )\] <p>This necessitates that the dimension of the original vector be even, which can be done by adding an additional dimension if the vector dimension is odd.</p> <p>We then inject positional information via rotation, which looks something like this:</p> \[f^{RoPE}(\mathbf{x}_m, m)_{j} = (x^{(2j)} + ix^{(2j+1)})e^{mi\theta_j}\] <p>Or in rotational matrix form:</p> \[f^{RoPE}(\mathbf{x}_m, m)_{j} = \begin{bmatrix} \cos(m\theta_j) &amp; -\sin(m\theta_j)\\ \sin(m\theta_j) &amp; \cos(m\theta_j) \end{bmatrix} \begin{bmatrix} x^{(2j)} \\ x^{(2j+1)} \end{bmatrix}\] <p>\(\theta_j\) is the sinusoidal wave for dimension \(j\) and is defined to be \(\theta_j = 10000^{-2j/d}\). As a shorthand, we can let \(R_{\theta_j, m}\) be the rotation matrix with the parameters \(\theta_j\) and \(m\).</p> <ul> <li> <p>The more general form of RoPE is given as</p> \[f^{RoPE}(\mathbf{x_m}, m) = R_{\Theta, m}\mathbf{x_m}\] <p>with the rotational matrix \(R_{\Theta, m}\) defined as:</p> \[R_{\Theta, m}= \begin{bmatrix} R_{\theta_0, m} &amp; &amp; &amp;\\ &amp; R_{\theta_1, m} &amp; \\ &amp; &amp; \ddots &amp;\\ &amp; &amp; &amp; R_{\theta_{d/2-1}, m}\\ \end{bmatrix}\] </li> </ul> </li> <li> <p>The above formulation of RoPE can then satisfy \eqref{eq:relativeposition}. For simplicity, we show the 2D case where \(\mathbf{x_m}, \mathbf{x_n} \in \mathbb{R}^2\):</p> \[\begin{align} \left&lt; f_q(\mathbf{x_m}, m), f_k(\mathbf{x_n}, n)\right&gt;_\mathbb{R} &amp;= \text{Re}(\left&lt; f^{RoPE}_q(\mathbf{x_m}, m), f^{RoPE}_k(\mathbf{x_n}, n)\right&gt;_\mathbb{C})\\ &amp;= \text{Re}(\hat{x}^*_m e^{-im\theta} \hat{x}_ne^{in\theta})\\ &amp;= \text{Re}\Big(\big((x^{(1)}_mx^{(1)}_n + x^{(2)}_mx^{(2)}_n) + i(-x^{(1)}_mx^{(2)}_n + x^{(2)}_mx^{(1)}_n)\big) e^{i(n-m)\theta}\Big)\\ &amp;= \cos\big((n-m)\theta\big) (x^{(1)}_mx^{(1)}_n + x^{(2)}_mx^{(2)}_n) \\ &amp;\quad\quad - \sin\big((n-m)\theta\big)(-x^{(1)}_mx^{(2)}_n + x^{(2)}_mx^{(1)}_n)\\ &amp;= \mathbf{x}^\intercal_\mathbf{m} R_{\theta, n-m} \mathbf{x_n}\\ &amp;= g(\mathbf{x_m}, \mathbf{x_n}, n-m) \end{align}\] <ul> <li>A big part of my confusion stems from the fact that this equivalence is given as true: \(\mathbf{q}\mathbf{k} = \text{Re}(\mathbf{q}^* \mathbf{k})\), which it is but I didn’t know how it got there since the vectors \(\mathbf{q}, \mathbf{k}\) are reused and the authors are relying on the readers to implicitly know if the vector is \(\mathbb{R}^d\) or \(\mathbb{C}^{d/2}\). If one isn’t being careful and assumed both vectors are in \(\mathbb{R}^d\), then one can observe that the complex conjugate of a real vector is itself, so \(qk = q^*k\), which means \(qk = \text{Re}(q^*k)\), but this isn’t the point being made in these papers.</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="machine-learning"/><category term="nlp"/><category term="nlp"/><category term="transformer"/><category term="machine-learning"/><summary type="html"><![CDATA[Notes about RoPE]]></summary></entry><entry><title type="html">Hello World</title><link href="https://rickysrcheng.github.io/blog/2023/first-post/" rel="alternate" type="text/html" title="Hello World"/><published>2023-10-20T17:30:00+00:00</published><updated>2023-10-20T17:30:00+00:00</updated><id>https://rickysrcheng.github.io/blog/2023/first%20post</id><content type="html" xml:base="https://rickysrcheng.github.io/blog/2023/first-post/"><![CDATA[<p>Hello! Welcome to my blog/website. I really like the idea of maintaining a personal space where I can just log my thoughts and learnings in a single place that’s convenient for me to revisit. However, I just never really gotten around to committing to one. I initially started using Github pages with the Hugo PaperMod theme. While the theme is good, I felt that it was missing a few features so I didn’t really stick with it. And then I found the al-folio theme on Jekyll and liked it enough to restart my blog.</p>]]></content><author><name></name></author><category term="blog"/><summary type="html"><![CDATA[first post]]></summary></entry></feed>