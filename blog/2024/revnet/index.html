<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> [P] The Reversible Residual Network | Ricky Cheng </title> <meta name="author" content="Ricky Cheng"> <meta name="description" content="Notes about RevNet"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pseudocode@2.4.1/build/pseudocode.min.css" crossorigin="anonymous"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rickysrcheng.github.io/blog/2024/revnet/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Ricky Cheng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">[P] The Reversible Residual Network</h1> <p class="post-meta"> April 26, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning</a>   <a href="/blog/tag/backpropagation"> <i class="fa-solid fa-hashtag fa-sm"></i> backpropagation</a>   <a href="/blog/tag/reversible-architecture"> <i class="fa-solid fa-hashtag fa-sm"></i> reversible-architecture</a>     ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning</a>   <a href="/blog/category/paper-reading"> <i class="fa-solid fa-tag fa-sm"></i> paper-reading</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="motivation">Motivation</h2> <ul> <li>ResNet advanced the state of deep learning and allowed for much deeper and wider deep neural networks to be trained</li> <li>However, as networks get bigger, memory consumption increases due to the need to store layer activations during forward pass for gradient updates in backpropagation</li> <li>May be beneficial to trade compute time for memory space, especially in systems where GPU memory is limited or complex to manage (such as multi-GPU distributed training)</li> </ul> <h2 id="background-materials">Background Materials</h2> <h4 id="backpropagation-and-saving-activation-values">Backpropagation and Saving Activation Values</h4> <ul> <li>Backpropagation is a technique used to compute gradient of a cost function with respect to a network node. Essentially, backpropagation <em>propagates</em> the error gradient from the cost function back into the gradient of the network. These gradients are then used by an optimization algorithm, such as stochastic gradient descent, to update the network parameters. The details of backpropagation can be found in numerous online sources, thus I won’t go into details here.</li> <li>The main issue the paper tries to tackle is that gradients of activation values require either the activation values or pre-activation values to compute <ul> <li>The popular ReLU activation function \(g(x) = \text{max}(0, x)\) has the derivative \(g'(x) = \begin{cases} 0 &amp; x &lt; 0 \\ 1 &amp; x &gt; 0\end{cases}\). This requires knowledge of the pre-activation values \(x\).</li> </ul> </li> <li>A technique used to reduce the amount of memory used to save activation values is called <em>gradient checkpointing</em>, which saves activation values every few layers and recomputes activation values for layers that don’t have it saved. This trades memory for compute time.</li> </ul> <h4 id="resnet">ResNet</h4> <ul> <li>Introduced in 2016 by <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" rel="external nofollow noopener" target="_blank">He et al</a>, ResNet is an architecture that uses residual learning blocks that made it possible to train deeper networks.</li> </ul> <div class="row mt-3 justify-content-center"> <div class="col-sm mt-3 mt-md-0" style="max-width:500px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/revnet-resnet-480.webp 480w,/assets/img/revnet-resnet-800.webp 800w,/assets/img/revnet-resnet-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/revnet-resnet.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <p>ResNets are built out of residual blocks, which takes the form of</p> \[y = x + \mathcal{F}(x)\] <p>where \(\mathcal{F}(x)\) is called a residual function, typically a small and shallow neural network. The skip connection allows information to flow freely through the network, which alleviates the vanishing gradient problem.</p> </li> </ul> <h4 id="reversible-architectures">Reversible Architectures</h4> <ul> <li>Reversible architectures are models that can return the input given only the model itself and the outputs, i.e. \(y = f(x)\) and \(x = f^{-1}(y)\).</li> <li>The architecture that inspired the authors is from <a href="https://arxiv.org/pdf/1410.8516.pdf" rel="external nofollow noopener" target="_blank">non-linear independent components estimation (NICE)</a>. The</li> <li> <p>Given \(x\), we partition \(x\) into \((x_1, x_2)\) along some dimension. Then we get the following forward mapping:</p> \[\begin{align*} y_1 &amp; = x_1 \\ y_2 &amp; = x_2 + \mathcal{F}(x_1) \end{align*}\] <p>We can get the input \(x_1, x_2\) back if we have the outputs \(y_1, y_2\) using the following inverse mapping:</p> \[\begin{align*} x_1 &amp; = y_1 \\ x_2 &amp; = y_2 - \mathcal{F}(y_1) \end{align*}\] </li> </ul> <h2 id="reversible-resnet">Reversible ResNet</h2> <ul> <li>In <a href="https://arxiv.org/abs/1707.04585" rel="external nofollow noopener" target="_blank">RevNet</a>, the authors proposed incorporating a reversible architecture into ResNet. The reversibility allows each block’s pre-activations to be calculated directly from the next layer’s activations, thereby saving memory.</li> </ul> <h4 id="architecture">Architecture</h4> <div class="row mt-3 justify-content-center"> <div class="col-sm mt-3 mt-md-0" style="max-width:550px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/revnet-arch-480.webp 480w,/assets/img/revnet-arch-800.webp 800w,/assets/img/revnet-arch-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/revnet-arch.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <p>RevNet is built out of reversible residual blocks. Given an input \(x\), we first partition \(x\) into \((x_1, x_2)\) along a dimension (the paper chose to partition along the channel). The resulting output is given as:</p> \[\begin{align*} y_1 &amp; = x_1 + \mathcal{F}(x_2)\\ y_2 &amp; = x_2 + \mathcal{G}(y_1) \end{align*}\] <p>where the functions \(\mathcal{F}(\cdot)\) and \(\mathcal{G}(\cdot)\) are analogous to the residual functions found in ResNet.</p> <p>The inverse mapping, similar to NICE, is given by:</p> \[\begin{align*} x_1 &amp; = y_1 - \mathcal{F}(x_2)\\ x_2 &amp; = y_2 - \mathcal{G}(y_1) \end{align*}\] </li> <li> <p>Unlike residual blocks, reversible blocks must have a stride of 1. Otherwise information is discarded, which makes the layer irreversible. Activations still needs to be explicitly stored for non-reversible layers.</p> </li> </ul> <h4 id="backpropagation">Backpropagation</h4> <ul> <li>The backpropagation for RevNet is given below. The paper used \(\bar{v}_i\) to denote the total derivative of node \(v_i\) with respect to the cost function \(\mathcal{C}\), \(\bar{v}_i = d\mathcal{C}/d v_i\). <pre><code class="language-pseudocode">\begin{algorithm}
\caption{RevNet Backprop}
\begin{algorithmic}
\PROCEDURE{RevNetBackprop}{$ (y_1, y_2), (\bar{y}_1, \bar{y}_2) $}
  \STATE \COMMENT{Retrieve input activation}
  \STATE $x_2 \gets y_2 - \mathcal{G}(y_1)$
  \STATE $x_1 \gets y_1 - \mathcal{F}(x_2)$
  \STATE \COMMENT{Calculate activation gradient}
  \STATE $\bar{x}_1 \gets \bar{y}_1 + (\frac{\partial \mathcal{G}}{\partial y_1})^\intercal\bar{y}_2$
  \STATE $\bar{x}_2 \gets \bar{y}_2 + (\frac{\partial \mathcal{F}}{\partial y_2})^\intercal \bar{x}_1$
  \STATE \COMMENT{Calculate parameter gradient}
  \STATE $\bar{\theta}_\mathcal{F} \gets (\frac{\partial \mathcal{F}}{\partial \theta_{\mathcal{F}}})^\intercal \bar{x}_1$
  \STATE $\bar{\theta}_\mathcal{G} \gets (\frac{\partial \mathcal{G}}{\partial \theta_{\mathcal{G}}})^\intercal \bar{y}_2$
  \RETURN $(x_1, x_2), (\bar{x}_1, \bar{x}_2), (\bar{\theta}_\mathcal{F}, \bar{\theta}_\mathcal{G})$
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre> </li> <li>In contrast, the algorithm of ResNet is given below. Note that it requires \(x\) be given as input: <pre><code class="language-pseudocode">\begin{algorithm}
\caption{RevNet Backprop}
\begin{algorithmic}
\PROCEDURE{RevNetBackprop}{$ x, y, \bar{y} $}
  \STATE $\bar{x} \gets \bar{y} + (\frac{\partial F}{\partial x})^\intercal \bar{y}$
  \STATE $\bar{\theta}_\mathcal{F} \gets (\frac{\partial F}{\partial \theta_\mathcal{F}})^\intercal \bar{y}$
  \RETURN $\bar{x}, \bar{\theta}_\mathcal{F}$
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre> </li> </ul> <h2 id="experiments">Experiments</h2> <ul> <li>Since RevNet blocks have around twice the computation depth of a ResNet block. Thus, the RevNet models have approximately half the number of blocks and twice the number of channels than its ResNet counterpart.</li> <li>The experiments showed that RevNet performs similarly to ResNet in all of its classification tests and also matches the training dynamics.</li> <li>The paper doesn’t cite a figure for how much memory saving was achieved. However, the authors did claim that, using the same GPU resources, RevNet was able to train on a mini-batch size of 128 images, whereas its ResNet counterpart was only able to train on mini-batch size of 32 images.</li> </ul> <h2 id="references">References</h2> <ul> <li>The Reversible Residual Network: Backpropagation Without Storing Activations, Gomez et al., NeurIPS 2017</li> <li>NICE: Non-linear Independent Components Estimation, Dinh et al., ICLR 2015</li> <li>Deep Residual Learning for Image Recognition, He et al., CVPR 2016</li> </ul> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Ricky Cheng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{inlineMath:[["$$","$$"],["\\(","\\)"]],displayMath:[["$","$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0}};</script> <script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/pseudocode@2.4.1/build/pseudocode.min.js"></script> <script>document.onreadystatechange=(()=>{"complete"===document.readyState&&(document.querySelectorAll("pre>code.language-pseudocode").forEach(e=>{const t=e.textContent,d=e.parentElement.parentElement;let n=document.createElement("pre");n.classList.add("pseudocode");const o=document.createTextNode(t);n.appendChild(o),d.appendChild(n),d.removeChild(e.parentElement),pseudocode.renderElement(n)}),MathJax.typeset())});</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>