<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> [P] PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications | Ricky Cheng </title> <meta name="author" content="Ricky Cheng"> <meta name="description" content="Notes about PipeSwitch"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rickysrcheng.github.io/blog/2024/pipeswitch/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Ricky Cheng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">[P] PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications</h1> <p class="post-meta"> March 21, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/mlsys"> <i class="fa-solid fa-hashtag fa-sm"></i> mlsys</a>   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning</a>     ·   <a href="/blog/category/mlsys"> <i class="fa-solid fa-tag fa-sm"></i> mlsys</a>   <a href="/blog/category/paper-reading"> <i class="fa-solid fa-tag fa-sm"></i> paper-reading</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="motivation">Motivation</h2> <ul> <li>Deep learning tasks have two primary workloads: training and inference. <ul> <li>Training workloads are throughput intensive but are more flexible in latency, since it typically takes a while to finish.</li> <li>Inference workloads are latency sensitive and have uncertain workloads.</li> </ul> </li> <li>Typical practice is to provision separate GPU clusters for training and inference, which leads to inefficiencies <ul> <li>Inference clusters are often over-provisioned to satisfy peak workload</li> <li>Training workloads cannot use inference clusters when inference load is low</li> </ul> </li> <li>Ideally, unify resources and use one cluster for both tasks via time-sharing <ul> <li>Problem: overhead for context switching is high for GPUs, which impacts inference latency requirements</li> </ul> </li> </ul> <h2 id="pipeswitch">PipeSwitch</h2> <ul> <li>PipeSwitch is a system proposed by <a href="https://www.usenix.org/conference/osdi20/presentation/bai" rel="external nofollow noopener" target="_blank">Zhihao Bai et al</a> to allow efficient time-sharing of a GPU</li> </ul> <div class="row mt-3 justify-content-center"> <div class="col-sm mt-3 mt-md-0" style="max-width:400px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pipeswitch-arch-480.webp 480w,/assets/img/pipeswitch-arch-800.webp 800w,/assets/img/pipeswitch-arch-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/pipeswitch-arch.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> PipeSwitch Architecture </div> <ul> <li>PipeSwitch consists of the following subsystems: <ul> <li>Controller: Responsible for handling client requests, scheduling workers to the GPU, and directing the memory daemon to allocate and transfer model weights</li> <li>Memory Daemon: Allocates GPU memory to the active worker and transfers model weights between host and GPU memory</li> <li>Active/Standby workers: Active workers are workers currently executing a task in the GPU. Standby workers are workers are workers who may be idle, initializing a new task, or cleaning up a previous task</li> </ul> </li> </ul> <h2 id="design-mechanisms">Design Mechanisms</h2> <p>PipeSwitch introduces three main mechanisms that work together to reduce context switching overhead</p> <h4 id="pipelined-model-transmisson">Pipelined Model Transmisson</h4> <div class="row mt-3 justify-content-center"> <div class="col-sm mt-3 mt-md-0" style="max-width:450px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/pipeswitch-pipeline-480.webp 480w,/assets/img/pipeswitch-pipeline-800.webp 800w,/assets/img/pipeswitch-pipeline-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/pipeswitch-pipeline.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Pipelined model transmission can shorten latency and have better utilization of GPU resources </div> <ul> <li>Deep learning models employ a layered architecture which means that not all layer weights need to be loaded into memory to begin execution. This observation means that we can pipeline the model execution and data transmission <ul> <li>However, GPU cores executes calculations much faster than memory operations. Thus, need to consider the granularity. <ul> <li>Whole model granularity is the same as loading the whole model into memory before execution.</li> <li>Per layer granularity operates on one layer at a time. However, this may incur significant overhead due to PCIe costs and synchronization costs.</li> </ul> </li> </ul> </li> <li>To this end, the authors propose <em>model-aware grouping</em> to find the optimal grouping to transmit and execute. The algorithm considers both the layer and number of layers to group together. The algorithm and proof, which I will not go into, is provided in the paper for those who are interested.</li> </ul> <h4 id="unified-memory-management">Unified Memory Management</h4> <ul> <li> <p>Since DL tasks require GPU memory, the authors proposed a unified memory management system to reduce overhead from allocating and transmitting the models. This is because the memory footprint of the model parameters remain unchanged during task execution. Neither forward pass nor backpropagation will change the model structure. In addition, while intermediate results are needed during training, they are produced and consumed in a structured and predictable manner. Thus, a general purpose memory management system, like what CUDA provides, is too heavyweight.</p> </li> <li> <p>The unified memory management has four mechanisms that helps to reduce memory overhead:</p> <ol> <li> <strong>Minimize Memory Allocation Overhead</strong>: Essentially, the memory daemon sits on top of CUDA and obtains GPU memory on startup. This eliminates the need for each task worker to allocate its own GPU memory. Instead, the daemon, having already obtained GPU memory, will pass a pointer to the worker, saving overhead. This also allows the daemon to guarantee memory isolation between workers.</li> <li> <strong>Minimize memory footprint and avoid extra memory copies</strong>: It may be a case that the same model is needed for multiple tasks. Having each task have a duplicate copy wastes memory spaces. However, having a separate process to save models in host memory would incur memory overhead from transferring the model to the task to transmit to the GPU. The memory daemon solves both as it keeps one copy of each model in host memory. Since it manages both host and GPU memory, it can also transmit the model to GPU directly.</li> <li> <strong>Minimize IPC Overhead</strong>: I’m not too familiar with GPU IPCs and the associated overheads; though I still tried my best in understanding this mechanism. <br> Since the model transmission is pipelined, synchronization needs to occur between the memory daemon and the worker. However, using GPU IPC is expensive. However, the authors observed a property that memory allocation for a neural network model is <em>deterministic</em>. So, given the same model and GPU memory region, the memory pointers for each pipeline group would be the same as long as the allocation order is the same between the memory daemon and the worker. Thus, we can take advantage of this and use CPU IPCs, which are cheap, in place of GPU IPCs to signal which pipeline group is transmitted.</li> <li> <strong>Pin Memory</strong>: GPUs require a page to be pinned in host memory for memory transmission. If no page is pinned in host memory, a temporary page is pinned for transmission. The authors thus pin the pages of the memory daemon to avoid such overhead.</li> </ol> </li> </ul> <h4 id="active-standby-worker-switching">Active-Standby Worker Switching</h4> <ul> <li>Using separate processes for tasks incur high overhead for initialization and cleanup of the GPU environment</li> <li>Allowing multiple tasks to share one CUDA environment still incurs overhead for cleanup of the environment</li> <li>Thus, the authors propose an active-standby worker switching mechanism. <ul> <li>Each worker is a separate process with its own CUDA context. There will be a fixed number of workers, only one of which is active at anytime.</li> <li>All workers initialize its own CUDA context on startup.</li> <li>If the current active worker is stopped, it needs to clean up and free GPU memory. However, since the memory is managed by the memory daemon, cleanup consists of releasing the memory pointers.</li> </ul> </li> </ul> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Ricky Cheng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>